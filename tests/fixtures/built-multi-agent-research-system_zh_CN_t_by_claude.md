## 我们如何构建多 AI 智能体研究系统

> 我们的研究功能使用多个 Claude AI 智能体来更有效地探索复杂主题。我们分享在构建这个系统过程中遇到的工程挑战和学到的经验教训。

Claude 现在具备了[研究能力](https://www.anthropic.com/news/research)，能够跨网络、Google Workspace 以及各种集成平台进行搜索，从而完成复杂任务。

这个多 AI 智能体系统从原型到生产环境的开发历程，让我们在系统架构、工具设计和提示词工程方面获得了重要经验。多 AI 智能体系统由多个 AI 智能体（即大语言模型在循环中自主使用工具）协同工作组成。我们的研究功能包含一个根据用户查询制定研究计划的 AI 智能体，然后该智能体使用工具创建多个并行的 AI 智能体来同时搜索信息。多 AI 智能体系统在智能体协调、评估和可靠性方面带来了全新的挑战。

本文将详细介绍对我们行之有效的设计原则——希望这些原则能够为你构建自己的多 AI 智能体系统提供有益参考。

### 多智能体系统的优势

研究工作涉及开放式问题，很难提前预测所需的步骤。你无法为探索复杂主题硬编码固定路径，因为这个过程本质上是动态的且依赖路径。当人们进行研究时，往往会根据新发现持续调整方法，跟随调查过程中出现的线索。

这种不可预测性使得AI智能体特别适合研究任务。研究需要灵活性，能够在调查展开时转向或探索相关联系。模型必须自主运行多个回合，根据中间发现决定追求哪些方向。线性、一次性的处理流程无法胜任这些任务。

搜索的本质是压缩：从庞大的语料库中提炼洞察。子智能体通过在各自的上下文窗口中并行运作来促进压缩，同时探索问题的不同方面，然后为主研究智能体汇总最重要的Token。每个子智能体还实现了职责分工——使用不同的工具、提示词和探索轨迹——这减少了路径依赖，实现了彻底而独立的调查。

一旦智能达到某个阈值，多智能体系统就成为扩展性能的重要方式。例如，尽管在过去10万年中个体人类变得更加智能，但人类社会在信息时代因为集体智慧和协调能力而变得指数级更强大。即使是通用智能智能体在独立运作时也面临限制；智能体团队可以完成更多任务。

我们的内部评估显示，多智能体研究系统在广度优先查询（即同时追求多个独立方向的查询）方面表现特别出色。我们发现，以Claude Opus 4作为主智能体、Claude Sonnet 4作为子智能体的多智能体系统，在我们的内部研究评估中比单智能体Claude Opus 4的表现提高了90.2%。例如，当被要求识别信息技术行业S&P 500公司的所有董事会成员时，多智能体系统通过将任务分解给子智能体找到了正确答案，而单智能体系统在缓慢的顺序搜索中未能成功。

多智能体系统之所以有效，主要是因为它们能够投入足够的Token来解决问题。在我们的分析中，三个因素解释了BrowseComp评估中95%的性能差异（该评估测试浏览智能体定位难找信息的能力）。我们发现Token使用量本身解释了80%的差异，工具调用次数和模型选择是另外两个解释因素。这一发现验证了我们的架构设计：将工作分布到具有独立上下文窗口的智能体之间，为并行推理增加更多容量。最新的Claude模型大幅提升了Token使用效率，升级到Claude Sonnet 4比在Claude Sonnet 3.7上将Token预算翻倍带来更大的性能提升。多智能体架构有效地为超出单智能体限制的任务扩展了Token使用。

不过也有缺点：在实践中，这些架构会快速消耗Token。根据我们的数据，智能体通常使用约4倍于聊天交互的Token，多智能体系统使用约15倍于聊天的Token。为了保证经济可行性，多智能体系统需要任务价值足够高，能够承担增加的性能成本。此外，一些领域要求所有智能体共享相同上下文，或涉及智能体之间的大量依赖关系，目前并不适合多智能体系统。例如，大多数编码任务的真正可并行化程度比研究工作要低，而且大语言模型智能体还不擅长实时协调和委托任务给其他智能体。我们发现，多智能体系统在需要大量并行化、信息量超出单一上下文窗口以及需要与众多复杂工具交互的高价值任务中表现出色。

### 研究架构概览

我们的研究系统采用多智能体架构，使用编排器-工作器模式，其中一个主导智能体协调整个过程，同时将任务委派给并行运行的专门子智能体。

![The multi-agent architecture in action](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F1198befc0b33726c45692ac40f764022f4de1bf2-4584x2579.png&w=3840&q=75)
> 多 AI 智能体架构的实际应用：用户查询通过主 AI 智能体流转，主 AI 智能体创建专门的子 AI 智能体来并行搜索不同方面。

当用户提交查询时，主 AI 智能体会分析查询内容，制定策略，并生成子 AI 智能体来同时探索不同方面。如上图所示，子 AI 智能体作为智能过滤器，通过迭代使用搜索工具来收集信息（在这个例子中是关于2025年 AI 智能体公司的信息），然后将公司列表返回给主 AI 智能体，使其能够汇总出最终答案。

传统的检索增强生成（RAG）方法使用静态检索。也就是说，它们会获取与输入查询最相似的一些信息片段，并使用这些片段来生成响应。相比之下，我们的架构采用多步骤搜索，能够动态发现相关信息，适应新的发现，并分析结果以生成高质量的答案。

![The multi-agent system process diagram](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F3bde53c9578d74f6e05c3e515e20b910c5a8c20a-4584x4584.png&w=3840&q=75)
> 流程图展示了我们多智能体研究系统的完整工作流程。当用户提交查询时，系统会创建一个主研究智能体，该智能体进入迭代研究过程。主研究智能体首先思考研究方法并将计划保存到内存中以保持上下文信息，这一点很重要，因为当上下文窗口超过200,000个Token时会被截断，需要保留研究计划。然后它会创建专门的子智能体（图中显示了两个，但实际数量可以是任意的）来执行特定的研究任务。每个子智能体会独立执行网络搜索，使用[交替思考](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#interleaved-thinking)方法评估工具结果，并将研究发现返回给主研究智能体。主研究智能体整合这些结果并判断是否需要进行更多研究——如果需要，它可以创建更多子智能体或优化研究策略。一旦收集到足够的信息，系统退出研究循环，将所有研究发现传递给引用处理智能体，该智能体会处理文档和研究报告以识别具体的引用位置。这确保了所有声明都能正确归属到其来源。最终的研究结果连同完整的引用信息一起返回给用户。

### 研究 AI 智能体的提示词工程和评估

多智能体系统与单智能体系统存在关键差异，其中协调复杂性呈快速增长趋势。早期的 AI 智能体会出现一些错误，比如为简单查询创建50个子智能体，无休止地搜索网络寻找不存在的信息源，以及用过多的状态更新互相干扰。由于每个 AI 智能体都由提示词引导，提示词工程成为我们改善这些行为的主要手段。以下是我们在 AI 智能体提示方面学到的一些原则：

1. **像你的 AI 智能体一样思考。** 要优化提示词，必须理解它们的效果。为此，我们使用[控制台](https://console.anthropic.com/)构建了仿真环境，采用系统中完全相同的提示词和工具，然后逐步观察 AI 智能体的工作过程。这立即暴露了失败模式：AI 智能体在已获得充分结果后仍继续工作，使用过于冗长的搜索查询，或选择错误的工具。有效的提示依赖于建立 AI 智能体的准确心理模型，这样可以明确识别最具影响力的改进点。

2. **教会主控智能体如何委派任务。** 在我们的系统中，主 AI 智能体将查询分解为子任务并向子智能体描述这些任务。每个子智能体都需要明确的目标、输出格式、工具和信息源使用指导，以及清晰的任务边界。缺乏详细任务描述时，AI 智能体会重复工作、留下空白或无法找到必要信息。我们最初允许主 AI 智能体给出简单、简短的指令，如"研究半导体短缺"，但发现这些指令往往过于模糊，导致子智能体误解任务或与其他 AI 智能体执行完全相同的搜索。例如，一个子智能体研究2021年汽车芯片危机，而另外两个重复调查当前2025年供应链情况，缺乏有效的分工协作。

3. **根据查询复杂性调整工作量。** AI 智能体难以判断不同任务的适当工作量，因此我们在提示词中嵌入了规模化规则。简单的事实查找只需1个 AI 智能体进行3-10次工具调用，直接比较可能需要2-4个子智能体各自进行10-15次调用，复杂研究可能需要超过10个子智能体且职责分工明确。这些明确的指导原则帮助主 AI 智能体有效分配资源，避免对简单查询的过度投入，这是我们早期版本中的常见失败模式。

4. **工具设计和选择至关重要。** AI 智能体-工具界面与人机界面同样重要。使用正确的工具不仅高效，往往还是必需的。例如，AI 智能体在网络上搜索只存在于 Slack 中的上下文信息注定会失败。通过[MCP 服务器](https://modelcontextprotocol.io/introduction)为模型提供外部工具访问时，这个问题更加复杂，因为 AI 智能体会遇到描述质量参差不齐的未知工具。我们为 AI 智能体提供了明确的启发式规则：例如，首先检查所有可用工具，使工具使用与用户意图匹配，使用网络搜索进行广泛的外部探索，或优先选择专用工具而非通用工具。糟糕的工具描述可能导致 AI 智能体走上完全错误的路径，因此每个工具都需要明确的用途和清晰的说明。

5. **让 AI 智能体自我改进。** 我们发现 Claude 4 模型可以成为优秀的提示词工程师。给定提示词和失败模式时，它们能够诊断 AI 智能体失败的原因并提出改进建议。我们甚至创建了工具测试 AI 智能体——当面对有缺陷的 MCP 工具时，它会尝试使用该工具，然后重写工具说明以避免失败。通过数十次工具测试，这个 AI 智能体发现了关键的细微差别和错误。这个改进工具易用性的过程使使用新说明的后续 AI 智能体任务完成时间减少了40%，因为它们能够避免大多数错误。

6. **先广后窄的搜索策略。** 搜索策略应该模仿专业人员的研究方法：在深入具体细节前先探索整体情况。AI 智能体往往默认使用过长、过于具体的查询，导致返回结果很少。我们通过提示 AI 智能体从简短、广泛的查询开始，评估可用信息，然后逐步缩小搜索范围来克服这种倾向。

7. **指导思考过程。** [扩展思考模式](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking)使 Claude 在可见的思考过程中输出额外 Token，可作为可控的草稿空间。主 AI 智能体使用思考来规划方法，评估哪些工具适合任务，确定查询复杂性和子智能体数量，并定义每个子智能体的角色。我们的测试表明，扩展思考改善了指令遵循、推理和效率。子智能体也会进行规划，然后在工具调用结果后使用[交错思考](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking#interleaved-thinking)来评估质量、识别差距并优化下一个查询。这使子智能体在适应各种任务方面更加有效。

8. **并行工具调用显著提升速度和性能。** 复杂研究任务自然需要探索多个信息源。我们早期的 AI 智能体执行序列化搜索，速度极其缓慢。为提高速度，我们引入了两种并行化方式：(1) 主 AI 智能体并行而非串行地启动3-5个子智能体；(2) 子智能体并行使用3个以上的工具。这些改变将复杂查询的研究时间最多减少了90%，使 Research 能够在几分钟而非几小时内完成更多工作，同时覆盖比其他系统更丰富的信息。

我们的提示策略专注于培养良好的启发式方法而非严格规则。我们研究了专业人员处理研究任务的方法，并将这些策略编码到提示词中——包括将复杂问题分解为较小任务、仔细评估信息源质量、根据新信息调整搜索方法，以及识别何时应专注于深度研究（详细调查单一主题）还是广度研究（并行探索多个主题）等策略。我们还通过设置明确的防护措施主动缓解意外副作用，防止 AI 智能体失控。最后，我们专注于构建具有可观测性和测试用例的快速迭代循环。

### 智能体的有效评估

良好的评估对于构建可靠的AI应用程序至关重要，智能体也不例外。然而，评估多智能体系统面临着独特的挑战。传统的评估通常假设AI每次都遵循相同的步骤：给定输入X，系统应该遵循路径Y来产生输出Z。但多智能体系统不是这样工作的。即使具有相同的起点，智能体也可能采取完全不同的有效路径来达到目标。一个智能体可能搜索三个来源，而另一个搜索十个，或者它们可能使用不同的工具来找到相同的答案。由于我们并不总是知道正确的步骤是什么，通常无法仅仅检查智能体是否遵循了我们预先规定的"正确"步骤。相反，我们需要灵活的评估方法，判断智能体是否在遵循合理流程的同时实现了正确的结果。

**立即开始用小样本进行评估。** 在早期智能体开发阶段，变化往往会产生巨大影响，因为存在大量显而易见的改进机会。提示词的调整可能会将成功率从30%提升到80%。在效果如此明显的情况下，您只需几个测试用例就能发现变化的影响。我们从大约20个代表真实使用模式的查询集合开始。测试这些查询通常能让我们清楚地看到变化带来的影响。我们经常听到AI开发团队延迟创建评估，因为他们认为只有包含数百个测试用例的大型评估才有用。然而，最好是立即开始小规模测试，使用几个例子，而不是延迟到可以构建更全面的评估体系。

**LLM作为评判者的评估方法在执行得当时具有良好的扩展性。** 研究输出很难通过程序化方式评估，因为它们是自由形式的文本，很少有单一的正确答案。LLM非常适合对输出进行评分。我们使用了一个LLM评判者，根据评分标准中的各项指标评估每个输出：事实准确性（声明是否与来源匹配？），引用准确性（引用的来源是否与声明匹配？），完整性（是否涵盖了所有要求的方面？），来源质量（是否使用了主要来源而不是较低质量的次要来源？），以及工具效率（是否合理地使用了正确的工具？）。我们尝试了多个评判者来评估每个组件，但发现使用单个LLM调用和单个提示词输出0.0-1.0分数和通过/失败等级是最一致且与人类判断最符合的方法。当评估测试用例确实有明确答案时，这种方法特别有效，我们可以使用LLM评判者简单地检查答案是否正确（即，它是否准确列出了研发预算最多的前3家制药公司？）。使用LLM作为评判者使我们能够可扩展地评估数百个输出。

**人工评估能够发现自动化评估遗漏的问题。** 测试智能体的人员会发现评估系统遗漏的边缘情况，包括对异常查询的虚假回答、系统故障或微妙的来源选择偏见。在我们的案例中，人工测试人员注意到我们早期的智能体始终选择SEO优化的内容农场（为搜索引擎优化而大量生产内容的网站），而不是权威但排名较低的来源，如学术PDF或个人博客。在我们的提示词中添加来源质量评估规则有助于解决这个问题。即使在自动化评估的时代，人工测试仍然是必不可少的。

多智能体系统具有涌现性行为，这些行为在没有特定编程的情况下自然产生。例如，对主导智能体的小幅调整可能会不可预测地改变子智能体的行为方式。成功需要理解交互模式，而不仅仅是个体智能体的行为。因此，这些智能体的最佳提示词不仅仅是严格的指令，而是协作框架，定义了分工、问题解决方法和资源分配。要做到这一点，需要依靠精心设计的提示词和工具、可靠的启发式方法、良好的可观察性以及紧密的反馈循环。请参阅[我们Cookbook中的开源提示词](https://github.com/anthropics/anthropic-cookbook/tree/main/patterns/agents/prompts)以获取我们系统中的示例提示词。

### 生产可靠性和工程挑战

在传统软件中，一个错误可能会破坏功能、降低性能或导致系统中断。在AI智能体系统中，微小的变化会产生连锁反应，导致巨大的行为变化，这使得为必须在长期运行过程中维护状态的复杂智能体编写代码变得极其困难。

**智能体具有状态性，错误会累积叠加。** 智能体可以长时间运行，在众多工具调用过程中维护状态。这意味着我们需要可靠地执行代码并处理过程中的错误。如果没有有效的缓解措施，微小的系统故障对智能体来说可能是灾难性的。当错误发生时，我们不能简单地从头重启：重启既昂贵又会让用户感到沮丧。相反，我们构建了能够从智能体出错位置恢复的系统。我们还利用模型的智能来优雅地处理问题：例如，让智能体知道工具何时失效并让其自行适应，这种方法效果非常好。我们将基于Claude构建的AI智能体的适应性与重试逻辑和定期检查点等确定性保护措施相结合。

**调试需要采用新方法。** 智能体会做出动态决策，即使使用相同的提示词，在不同运行之间也具有非确定性。这使得调试变得更加困难。例如，用户会报告智能体"找不到明显的信息"，但我们无法看出原因。是智能体使用了糟糕的搜索查询吗？选择了不良来源？还是遇到了工具故障？通过添加完整的生产追踪，我们能够诊断智能体失败的原因并系统性地修复问题。除了标准的可观测性之外，我们还监控智能体的决策模式和交互结构——所有这些都不涉及监控个别对话的内容，以维护用户隐私。这种高层次的可观测性帮助我们诊断根本原因、发现意外行为并修复常见故障。

**部署需要精心协调。** AI智能体系统是由提示词、工具和执行逻辑组成的具有复杂状态的网络，几乎持续运行。这意味着每当我们部署更新时，智能体可能处于其流程中的任何位置。因此，我们需要防止代码更新破坏现有的智能体。我们无法同时将所有智能体更新到新版本。相反，我们使用[彩虹部署](https://brandon.dimcheff.com/2018/02/rainbow-deploys-with-kubernetes/)来避免干扰正在运行的智能体，通过逐渐将流量从旧版本转移到新版本，同时保持两个版本同时运行。

**同步执行造成瓶颈。** 目前，我们的主智能体采用同步方式执行子智能体，等待每组子智能体完成后再继续进行。这简化了协调工作，但在智能体之间的信息流中造成了瓶颈。例如，主智能体无法引导子智能体，子智能体之间无法协调，整个系统可能会因为等待单个子智能体完成搜索而被阻塞。异步执行将实现更多的并行处理：智能体可以并发工作，并在需要时创建新的子智能体。但这种异步性在结果协调、状态一致性和子智能体间的错误传播方面带来了挑战。随着模型能够处理更长、更复杂的研究任务，我们预期性能收益将证明这种复杂性是值得的。

### 结论

在构建 AI 智能体时，最后一步往往占据了整个开发过程的大部分时间。在开发者机器上能够运行的代码库，需要经过大量工程改造才能成为可靠的生产系统。智能体系统中错误的复合特性意味着，对传统软件而言的小问题，可能会彻底破坏智能体的运行。单个步骤的失败可能导致智能体探索完全不同的执行路径，从而产生不可预测的结果。基于本文描述的所有原因，原型与生产环境之间的差距往往比预期的更大。

尽管面临这些挑战，多智能体系统已被证明在开放式研究任务中极具价值。用户反馈称，Claude 帮助他们发现了此前未曾考虑的商业机会，应对复杂的医疗保健选择，解决棘手的技术问题，并通过发现他们独自无法找到的研究关联而节省了数天的工作时间。通过精心的工程设计、全面的测试、细致入微的提示词和工具设计、稳健的运营实践，以及研究、产品和工程团队之间的紧密协作——这些团队对当前智能体能力有着深入理解，多智能体研究系统能够实现大规模可靠运行。我们已经看到这些系统正在改变人们解决复杂问题的方式。

![A Clio embedding plot](https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F09a90e0aca54859553e93c18683e7fd33ff16d4c-2654x2148.png&w=3840&q=75)
> 一张 [Clio](https://www.anthropic.com/research/clio) 数据可视化图展示了当今人们使用研究功能最常见的方式。最主要的应用场景包括：在各个专业领域开发软件系统（占比10%）、开发和优化专业技术内容（占比8%）、制定业务增长和收入增长策略（占比8%）、协助学术研究和教育材料开发（占比7%），以及研究和验证关于人物、地点或组织的信息（占比5%）。

### 附录

以下是多 AI 智能体系统的一些额外补充建议。

**对在多轮对话中改变状态的 AI 智能体进行终态评估。** 评估在多轮对话中修改持久状态的 AI 智能体面临独特挑战。与只读研究任务不同，每个动作都可能改变后续步骤的环境，产生传统评估方法难以处理的依赖关系。我们发现专注于终态评估而非逐轮分析能够取得成功。与其判断 AI 智能体是否遵循了特定流程，不如评估它是否达到了正确的最终状态。这种方法承认 AI 智能体可能找到通往同一目标的不同路径，同时仍能确保它们交付预期成果。对于复杂工作流程，应将评估分解为离散的检查点，在这些检查点应该发生特定的状态变化，而不是试图验证每个中间步骤。

**长时对话管理。** 生产级 AI 智能体经常需要进行跨越数百轮的对话，这需要精心设计的上下文管理策略。随着对话的延长，标准上下文窗口变得不够用，必须采用智能的压缩和记忆机制。我们实施了一些模式：AI 智能体在进行新任务之前会总结已完成的工作阶段，并将关键信息存储在外部记忆中。当上下文限制即将达到时，AI 智能体可以生成拥有全新上下文的子 AI 智能体，同时通过精心设计的交接机制保持连续性。此外，它们可以从记忆中检索存储的上下文（如研究计划），而不是在达到上下文限制时丢失之前的工作成果。这种分布式方法既能防止上下文溢出，又能在长时间交互中保持对话的连贯性。

**子 AI 智能体直接输出到文件系统以减少信息传递失真。** 对于某些类型的结果，子 AI 智能体的直接输出可以绕过主协调器，从而提高保真度和性能。与其要求子 AI 智能体通过主 AI 智能体传达所有内容，不如实施工件系统，让专业的 AI 智能体能够创建独立持久化的输出。子 AI 智能体调用工具将其工作成果存储在外部系统中，然后将轻量级的引用传回协调器。这样既能防止多阶段处理过程中的信息丢失，又能减少通过对话历史复制大型输出所产生的 Token 开销。这种模式在处理结构化输出（如代码、报告或数据可视化）时效果特别好，因为在这些场景中，子 AI 智能体的专门提示词比通过通用协调器进行过滤能产生更好的结果。
